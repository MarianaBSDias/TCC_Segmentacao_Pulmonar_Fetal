






<!-- antes  de enviar a vers√£o final, solicitamos que todos os coment√°rios, colocados para orienta√ß√£o ao aluno, sejam removidos do arquivo -->
# Automatiza√ß√£o de Segmenta√ß√£o Pulmonar Fetal  Atrav√©s de Modelo de Rede Neural Convolucional U-Net 3D em Imagens de Resson√¢ncia Magn√©tica

#### Aluno: [Mariana Barros dos Santos Dias](https://github.com/MarianaBSDias/)
#### Orientadora: [Manoela Kohler](https://github.com/manoelakohler).
-->

---

Trabalho apresentado ao curso [BI MASTER](https://ica.puc-rio.ai/bi-master) como pr√©-requisito para conclus√£o de curso e obten√ß√£o de cr√©dito na disciplina "Projetos de Sistemas Inteligentes de Apoio √† Decis√£o".

<!-- para os links a seguir, caso os arquivos estejam no mesmo reposit√≥rio que este README, n√£o h√° necessidade de incluir o link completo: basta incluir o nome do arquivo, com extens√£o, que o GitHub completa o link corretamente -->
- [Link para o c√≥digo](https://github.com/link_do_repositorio). <!-- caso n√£o aplic√°vel, remover esta linha -->

- [Link para a monografia](https://link_da_monografia.com). <!-- caso n√£o aplic√°vel, remover esta linha -->

- Trabalhos relacionados: <!-- caso n√£o aplic√°vel, remover estas linhas -->
    - [Nome do Trabalho 1](https://link_do_trabalho.com).
    - [Nome do Trabalho 2](https://link_do_trabalho.com).

---

### Resumo

<!-- trocar o texto abaixo pelo resumo do trabalho, em portugu√™s -->

&nbsp; &nbsp; &nbsp; &nbsp; Nos √∫ltimos anos, a an√°lise de exames de imagens tridimensionais tornou-se um componente fundamental na pr√°tica cl√≠nica e na pesquisa biom√©dica. Exames como tomografia computadorizada (TC) e resson√¢ncia magn√©tica (RM) geram volumes ricos em informa√ß√£o, que permitem avaliar √≥rg√£os, tecidos e estruturas complexas com alta precis√£o. Entretanto, a segmenta√ß√£o manual dessas imagens √© uma tarefa extremamente demorada e pode apresentar inconsist√™ncias devido √† qualidade vari√°vel das imagens. O advento das redes neurais convolucionais (CNNs) e, especificamente, da arquitetura U-Net 3D, revolucionou o campo de segmenta√ß√£o de imagens m√©dicas, permitindo a an√°lise direta em volumes completos enquanto preserva a continuidade anat√¥mica das estruturas.

&nbsp; &nbsp; &nbsp; &nbsp; Este trabalho apresenta o desenvolvimento e avalia√ß√£o de um sistema automatizado para segmenta√ß√£o pulmonar fetal em imagens de resson√¢ncia magn√©tica 3D, utilizando a arquitetura U-Net 3D implementada no *framework* MONAI. O *pipeline* abrange desde o carregamento de dados NRRD at√© a gera√ß√£o de m√°scaras segmentadas, incluindo pr√©-processamento, normaliza√ß√£o, reamostragem para *voxel* isotr√≥pico de 1,5 mm¬≥, redimensionamento para 128¬≥ *voxels* e aumento de dados.

&nbsp; &nbsp; &nbsp; &nbsp; O modelo foi treinado em GPU NVIDIA com monitoramento do coeficiente de *Dice* e fun√ß√£o de perda combinada (*Dice* + *Cross Entropy*), alcan√ßando 0,7608 no conjunto de teste ‚Äî resultado considerado clinicamente relevante. O uso da t√©cnica de janela deslizante (*sliding window*) permitiu processar volumes completos mantendo consist√™ncia espacial.

&nbsp; &nbsp; &nbsp; &nbsp; Apesar do bom desempenho, a necessidade de reduzir a resolu√ß√£o das imagens para o treinamento limitou a fidelidade das m√°scaras e causou perda de detalhes na tentativa de reescalar as m√°scaras. Mesmo assim, foi poss√≠vel obter estimativas volum√©tricas √∫teis aplicando fator de escala. Como trabalho futuro, planeja-se treinar o modelo em resolu√ß√£o original com *hardware* mais potente, a fim de aumentar a precis√£o, utiliza√ß√£o das m√°scaras e preservar a riqueza de detalhes para aplica√ß√µes cl√≠nicas e de pesquisa.

<h4>Palavras-chave</h4>
<p>Segmenta√ß√£o Autom√°tica, Resson√¢ncia Magn√©tica Fetal, U-Net 3D, MONAI, Deep Learning, Processamento de Imagens M√©dicas, Pulm√£o Fetal.<span class="mark"></span></p>

---

### Abstract <!-- Opcional! Caso n√£o aplic√°vel, remover esta se√ß√£o -->

<!-- trocar o texto abaixo pelo resumo do trabalho, em ingl√™s -->

<p> &nbsp; &nbsp; &nbsp; &nbsp; In recent years, the analysis of three-dimensional imaging exams has become a fundamental component in clinical practice and biomedical research. Exams such as computed tomography (CT) and magnetic resonance imaging (MRI) generate information-rich volumes that allow for the evaluation of organs, tissues, and complex structures with high precision. However, the manual segmentation of these images is an extremely time-consuming task and can present inconsistencies due to the variable quality of the images. The advent of convolutional neural networks (CNNs) and, specifically, the U-Net 3D architecture, has revolutionized the field of medical image segmentation, allowing direct analysis of entire volumes while preserving the anatomical continuity of structures.  </p>

<p> &nbsp; &nbsp; &nbsp; &nbsp; This work presents the development and evaluation of an automated system for fetal lung segmentation in 3D magnetic resonance imaging, using the U-Net 3D architecture implemented in the MONAI framework. The pipeline covers everything from loading NRRD data to generating segmented masks, including preprocessing, normalization, resampling to 1.5 mm¬≥ isotropic voxels, resizing to 128¬≥ voxels, and data augmentation.  </p>

<p> &nbsp; &nbsp; &nbsp; &nbsp; The model was trained on an NVIDIA GPU with monitoring of the Dice coefficient and combined loss function (Dice + Cross Entropy), achieving 0.7608 in the test set ‚Äî a result considered clinically relevant. The use of the sliding window technique allowed processing entire volumes while maintaining spatial consistency.  </p>

<p> &nbsp; &nbsp; &nbsp; &nbsp; Despite the good performance, the need to reduce the image resolution for training limited the fidelity of the masks and caused a loss of detail when attempting to rescale them. Even so, it was possible to obtain useful volumetric estimates by applying scaling factor. As future work, it is planned to train the model at its original resolution with more powerful hardware in order to increase accuracy, mask utilization, and preserve the richness of detail for clinical and research applications.  </p>

<h4>Keywords</h4>
<p>Automatic Segmentation, Fetal Magnetic Resonance Imaging, U-Net 3D, MONAI, Deep Learning, Medical Image Processing, Fetal Lung.</p>

---

### 1. Introdu√ß√£o

Este trabalho de conclus√£o de curso (TCC) visa explorar, implementar e avaliar uma abordagem completa de segmenta√ß√£o de imagens m√©dicas 3D utilizando U-Net 3D. A pesquisa inclui todas as etapas necess√°rias: aquisi√ß√£o e pr√©-processamento de imagens, defini√ß√£o e treinamento do modelo, avalia√ß√£o quantitativa e qualitativa dos resultados e execu√ß√£o de infer√™ncia em novos volumes de teste.

#### 1.1 Objetivos

Os objetivos espec√≠ficos deste estudo incluem:

1.	Implementar um pipeline de pr√©-processamento adequado para imagens m√©dicas 3D, garantindo consist√™ncia espacial e intensidade padronizada.

2.	Treinar uma U-Net 3D adaptada para segmenta√ß√£o de uma estrutura espec√≠fica em volumes volum√©tricos, utilizando t√©cnicas de monitoramento de desempenho e salvamento do melhor modelo.

3.	Avaliar quantitativamente o desempenho do modelo em um conjunto de teste independente, calculando m√©tricas como Dice e desvio padr√£o.

4.	Realizar infer√™ncia em novas imagens, aplicando t√©cnicas de sliding window e p√≥s-processamento para obten√ß√£o de m√°scaras bin√°rias consistentes.

5.	Discutir os resultados, limita√ß√µes e potenciais aplica√ß√µes cl√≠nicas ou de pesquisa.

---

### 2. Modelagem

#### Arquitetura Geral do Sistema

O sistema implementa um **pipeline completo de processamento**:

**Aquisi√ß√£o de Resson√¢ncia Magn√©tica (RM)** ‚Üí **Pr√©-processamento** ‚Üí **Aumento de Dados** ‚Üí **Treinamento da U-Net 3D** ‚Üí **Valida√ß√£o** ‚Üí **Infer√™ncia**

---

#### 2.1. Base de Dados

A base de dados utilizada neste estudo consiste em **imagens m√©dicas volum√©tricas** no formato **NRRD** (*Nearly Raw Raster Data*), amplamente utilizado para armazenar dados tridimensionais de tomografia computadorizada (CT) e resson√¢ncia magn√©tica (MRI).  

O formato NRRD √© vantajoso por manter **metadados essenciais**, como espa√ßamento de voxels, orienta√ß√£o e dimens√µes originais, garantindo consist√™ncia no pr√©-processamento.

**Caracter√≠sticas do dataset:**
- **Total de volumes:** 342 exames  
- **Divis√£o:**
  - Treinamento: 260 volumes (76%)  
  - Valida√ß√£o: 47 volumes (14%)  
  - Teste: 35 volumes (10%)  

**Dimens√µes originais:** entre `384 √ó 384 √ó 176` voxels, variando em profundidade (*slices*) altura e largura.  
**Modalidade:** imagens em escala de cinza (*single-channel*), representando densidade ou intensidade do tecido.
**Distribui√ß√£o aleat√≥ria:** a divis√£o foi realizada garantindo representatividade para cada conjunto, evitando vieses na avalia√ß√£o do modelo. 

**Estrutura de diret√≥rios:**

Base_de_Dados/

‚îú‚îÄ‚îÄ Paciente_001/

‚îÇ ‚îú‚îÄ‚îÄ imagem_001.nrrd # Volume de RM original

‚îÇ ‚îî‚îÄ‚îÄ imagem_001.seg.nrrd # M√°scara de segmenta√ß√£o manual

‚îú‚îÄ‚îÄ Paciente_002/

‚îÇ ‚îú‚îÄ‚îÄ imagem_002.nrrd

‚îÇ ‚îî‚îÄ‚îÄ imagem_002.seg.nrrd

‚îî‚îÄ‚îÄ ...


A escolha desse dataset permitiu avaliar o desempenho do modelo em volumes complexos, com variabilidade anat√¥mica e de qualidade de imagem t√≠pica de exames cl√≠nicos.

**Formato NRRD:** padr√£o em neuroimagem, com suporte a metadados ricos.  
**Metadados inclu√≠dos:**
- Espa√ßamento de voxel (dimens√µes f√≠sicas)  
- Orienta√ß√£o anat√¥mica  
- Tipo de dados e codifica√ß√£o  

**Estrutura de arquivos:**
- `imagem_original.nrrd`: Dados de intensidade da RM  
- `imagem_original.seg.nrrd`: M√°scaras de segmenta√ß√£o manual  

---

#### 2.2. Hardware e Software

O treinamento e a infer√™ncia foram realizados em ambiente **GPU CUDA**, utilizando:

- **Python 3.10**  
- **PyTorch 2.x**  
- **MONAI 1.x** ‚Äî biblioteca especializada para *deep learning* em imagens m√©dicas  
- **nrrd** ‚Äî leitura e manipula√ß√£o de volumes NRRD  
- **Matplotlib** ‚Äî visualiza√ß√£o de *slices*

A escolha do MONAI se deve √† sua integra√ß√£o nativa com o PyTorch, oferecendo *transforms* e *inferers* prontos para imagens 3D.

---

#### 2.3. Pr√©-Processamento

O pr√©-processamento √© uma etapa cr√≠tica para padronizar volumes de diferentes exames, reduzir variabilidade e preparar os dados para a U-Net 3D (Kondrateva‚ÄØ*et‚ÄØal*.,‚ÄØ2022). As etapas realizadas foram:

1. **Carregamento do volume (LoadImaged):** arquivos NRRD foram convertidos em tensores PyTorch. Este passo assegura que os dados possam ser manipulados de forma eficiente em pipelines de aprendizado profundo. <br>
Considera√ß√µes t√©cnicas: <br>
   - Mant√©m dimens√µes originais.  
   - Permite leitura de metadados como *spacing* e orienta√ß√£o.
     
2. **Garante o canal como primeira dimens√£o (EnsureChannelFirstd):** o modelo espera entrada no formato `(C, D, H, W)`, sendo `C = 1` (volumes s√£o monocanais). Essa transforma√ß√£o evita erros de dimensionalidade e garante compatibilidade com camadas convolucionais 3D.
   
3. **Padroniza√ß√£o de espa√ßamento (Spacingd):** todos os volumes foram reamostrados para `1,5 √ó 1,5 √ó 1,5` mm¬≥, evitando distor√ß√µes anat√¥micas, assegurando consist√™ncia volum√©trica e melhor aprendizado de padr√µes 3D pela rede.

4. **Corre√ß√£o de orienta√ß√£o (Orientationd):** volumes convertidos para padr√£o RAS (Right-Anterior-Superior). Isso garante que:
   - O modelo aprenda localiza√ß√£o anat√¥mica relativa (direita/esquerda, superior/inferior);
   - Evita confus√µes em regi√µes sim√©tricas (como rins, pulm√µes, hemisf√©rios cerebrais), a corre√ß√£o garante que o modelo n√£o troque esquerda e direita. Para estruturas assim√©tricas (como f√≠gado ou ba√ßo), garante que elas estejam sempre no mesmo lado anat√¥mico, conforme a conven√ß√£o (f√≠gado √† direita, ba√ßo √† esquerda).
     
5. **Normaliza√ß√£o de intensidade (NormalizeIntensityd):** intensidades ajustadas para m√©dia zero e desvio padr√£o unit√°rio. Isso permite que o modelo:
   - Seja robusto a varia√ß√µes de intensidade entre scanners;
   - Aprenda padr√µes anat√¥micos sem vi√©s de intensidade absoluta.
  
6. **Redimensionamento (ResizeD):** volumes ajustados para `128 √ó 128 √ó 128` voxels, equilibrando detalhes anat√¥micos e limita√ß√µes de mem√≥ria GPU. Permite treinamento em GPU comum. Isso foi feito para permitir treinamento em GPU comum e reduzir o tempo computacional de infer√™ncia.
 
7. **Convers√£o final para tensor (EnsureTyped):** compat√≠vel com MONAI para treinamento e infer√™ncia, garantindo integra√ß√£o com *DataLoaders*, *pipelines* e fun√ß√µes de *loss*.

**Tabela 2.1 ‚Äî Resumo do Pr√©-Processamento:**  
<img width="767" height="209" alt="image" src="https://github.com/user-attachments/assets/d6d7fc77-624c-4f9f-b191-3a3090de644a" />

Ap√≥s essas transforma√ß√µes, todos os volumes apresentaram shape uniforme, pronto para entrada no modelo.

---

#### 2.4. Aumento de Dados (*Data Augmentation*)

Para aumentar a robustez do modelo e prevenir sobreajuste (*overfitting*), s√£o aplicadas transforma√ß√µes geom√©tricas aleat√≥rias:

- **Rota√ß√µes em m√∫ltiplos eixos**
  - **RandRotate90d**: Rota√ß√µes de 90¬∞, 180¬∞ ou 270¬∞  

- **Espelhamentos (*flips*) sagitais, coronais e axiais**
  - **RandFlipd**: Espelhamento aleat√≥rio nos tr√™s eixos espaciais  

- **Transla√ß√µes e escalas aleat√≥rias**
  - **RandCropByPosNegLabeld**: Recortes aleat√≥rios balanceados (1 positivo:1 negativo)


- **Amostragem Balanceada**
  - Garante representa√ß√£o equilibrada entre tecido pulmonar e fundo
  - Previne vi√©s em dire√ß√£o √† classe majorit√°ria

---

#### 2.5. Arquitetura da U-Net 3D

O modelo U-Net 3D foi implementado por meio do m√≥dulo monai.networks.nets.UNet da biblioteca MONAI. A defini√ß√£o dos par√¢metros arquiteturais considerou a resolu√ß√£o dos volumes de entrada, a capacidade de mem√≥ria da GPU e a complexidade anat√¥mica da estrutura de interesse.

O modelo foi inicializado com pesos aleat√≥rios e treinado do zero, utilizando monitoramento cont√≠nuo da m√©trica Dice no conjunto de valida√ß√£o. O melhor modelo foi automaticamente salvo com base no maior valor de Dice alcan√ßado, garantindo que o resultado final representasse o estado de treinamento de maior desempenho.

A U-Net 3D implementada segue a estrutura cl√°ssica proposta por Ronneberger *et al*. (2015), com adapta√ß√µes para o processamento de imagens volum√©tricas tridimensionais. Suas principais configura√ß√µes s√£o:


- **Entrada:** volume tridimensional `(C, D, H, W)`, com `C = 1`, representando imagens em escala de cinza; 
- **Sa√≠da:** m√°scara bin√°ria tridimensional com `C = 1`, correspondente √† estrutura segmentada; 
- **Filtros por n√≠vel do encoder:** `(8, 16, 32, 64)`, aumentando progressivamente conforme a profundidade da rede; 
- **Strides (*downsampling*):** `(2, 2, 2)`, utilizados para redu√ß√£o sistem√°tica da dimensionalidade espacial;  
- **Conex√µes de salto (*skip connections*):** preservam detalhes anat√¥micos aprendidos nas camadas iniciais;
- **Blocos residuais:** uma por n√≠vel, empregadas para evitar a degrada√ß√£o do gradiente e permitir aprendizado mais profundo;
- **Normaliza√ß√£o em lote (*Batch Normalization*) e fun√ß√£o de ativa√ß√£o ReLU:** aplicadas em todas as camadas, a fim de estabilizar o treinamento e acelerar a converg√™ncia. 

A arquitetura compreende tr√™s **componentes principais:**
1. sua estrutura em formato de ‚ÄúU‚Äù, que possibilita o contexto global de cada voxel dentro do volume;
2. o uso de convolu√ß√µes tridimensionais (3D), que mant√™m as rela√ß√µes espaciais entre fatias e capturam a coer√™ncia anat√¥mica;
3. e as conex√µes de salto, essenciais para a segmenta√ß√£o precisa de √≥rg√£os pequenos ou estruturas de baixo contraste.

Essa configura√ß√£o arquitetural permite ao modelo capturar informa√ß√µes volum√©tricas detalhadas, promovendo alto desempenho e robustez em tarefas de segmenta√ß√£o m√©dica tridimensional.

---

#### 2.6. Treinamento do Modelo

O treinamento do modelo U-Net 3D foi conduzido considerando princ√≠pios voltados √† efici√™ncia do aprendizado e √† robustez na generaliza√ß√£o dos resultados. O processo contemplou 450 √©pocas de treinamento, com execu√ß√£o preferencial em GPU (CUDA), explorando a acelera√ß√£o computacional oferecida pelo processamento paralelo em unidades gr√°ficas. Quando a GPU n√£o estava dispon√≠vel, o treinamento foi automaticamente realizado em CPU, assegurando compatibilidade e portabilidade do modelo entre diferentes ambientes computacionais.

O pipeline de dados seguiu o pr√©-processamento descrito na Se√ß√£o 2.3, no qual cada volume foi padronizado para espa√ßamento isotr√≥pico, orienta√ß√£o RAS (*Right‚ÄìAnterior‚ÄìSuperior*) e escala de intensidade normalizada. Essa uniformiza√ß√£o foi fundamental para reduzir discrep√¢ncias entre exames provenientes de diferentes protocolos de aquisi√ß√£o, favorecendo a consist√™ncia dos dados e a efici√™ncia do aprendizado da rede.

Devido ao elevado consumo de mem√≥ria associado ao processamento de volumes tridimensionais, foi adotado *batch size* igual a 1, permitindo o treinamento com volumes completos sem preju√≠zo √† integridade das informa√ß√µes espaciais.

A **fun√ß√£o de perda** utilizada foi a ***Dice Loss***, amplamente reconhecida como adequada para segmenta√ß√£o m√©dica bin√°ria, por penalizar de forma proporcional √† sobreposi√ß√£o entre a m√°scara predita e a m√°scara de refer√™ncia (*ground truth*). Essa caracter√≠stica torna a *Dice Loss* particularmente eficaz em cen√°rios de desbalanceamento de classes, nos quais a propor√ß√£o de *voxels* pertencentes √† estrutura de interesse √© significativamente menor em rela√ß√£o ao fundo.

Como **otimizador**, empregou-se o ***Adam***, com taxa de aprendizado adaptativa em seus par√¢metros padr√£o, selecionado por sua estabilidade num√©rica e capacidade de converg√™ncia r√°pida em arquiteturas profundas.

Durante o treinamento, a **m√©trica *Dice*** foi monitorada a cada √©poca no conjunto de valida√ß√£o. O modelo apresentando o melhor desempenho de valida√ß√£o foi automaticamente salvo (mecanismo de *checkpoint*), assegurando que o modelo final correspondesse ao estado de aprendizado de maior desempenho obtido durante o processo. O mecanismo de *early stopping* tamb√©m foi empregado para interromper o treinamento caso n√£o houvesse melhora significativa na m√©trica de valida√ß√£o ap√≥s um n√∫mero pr√©-definido de √©pocas, prevenindo *overfitting* e otimizando o uso de recursos computacionais.

---

#### 2.7. Infer√™ncia

A infer√™ncia em novos volumes seguiu os mesmos passos de pr√©-processamento aplicados ao treino.

1. **Carregamento do Volume:** arquivo NRRD lido com `nrrd` e processado pelos mesmos *transforms* do treinamento, garantindo consist√™ncia.  
2. **Adi√ß√£o de dimens√£o de batch:** tensor com shape `(1, 1, D, H, W)` para compatibilidade com o modelo 3D.  
3. **Sliding Window Inference:** Necess√°rio para processar volumes grandes que n√£o cabem na mem√≥ria GPU de uma s√≥ vez.  
   - **Tamanho da janela:** `(96, 96, 96)`  
   - **Sobreposi√ß√£o:** `25%`  
   - **Batch size:** `1`  
4. **Aplica√ß√£o de sigmoid:** converteu *logits* da sa√≠da em probabilidades entre 0 e 1.  
5. **Threshold de 0.5:** para binariza√ß√£o da m√°scara predita.  
6. **Resultados:**  
   - **Shape da m√°scara predita:** `(128, 128, 128)`  
   - **Percentual de voxels positivos:** depende do volume, mas o pipeline permite an√°lise quantitativa do volume segmentado.  
   - **Dice Score m√©dio (teste):** `0.7608`  
   - **Desvio padr√£o do Dice:** `0.0959`

---

### 3. Resultados

#### 3.1. Avalia√ß√£o Quantitativa

A avalia√ß√£o quantitativa do modelo de segmenta√ß√£o 3D foi realizada utilizando o ***Dice Score***.  A fun√ß√£o **DICE**, ou coeficiente de DICE, √© uma m√©trica padr√£o de sobreposi√ß√£o para segmenta√ß√£o bin√°ria m√©dica usada para avaliar a similaridade entre duas m√°scaras bin√°rias: a predita e a manual (*ground truth*) (Zou *et al*., 2004):

$$
DICE = \frac{2 \times |A \cap B|}{|A| + |B|}
$$

onde $$A$$ representa a m√°scara predita pelo modelo e $$B$$, a m√°scara manual (*ground truth*).  

Ou, no caso cont√≠nuo (valores entre 0 e 1):

$$
DICE = \frac{2 \sum_i p_i g_i}{\sum_i p_i + \sum_i g_i}
$$

onde:  
- $p_i$: valor predito pelo modelo (probabilidade de pertencimento √† classe "pulm√£o")  
- $g_i$: *Ground Truth* no *pixel* ou *voxel* $$i$$, ou seja, o r√≥tulo real da imagem de segmenta√ß√£o. Valor real (0 ou 1), sendo **1**, se o *pixel* $$i$$ pertence √† classe "pulm√£o" e **0**, caso contr√°rio.

Conforme destacado por Zou *et al*. (2004), o DSC (*Dice Similarity Coefficient* ou seja, Coeficiente de Similaridade de Dice) √© uma medida resumo simples e √∫til de sobreposi√ß√£o espacial, que pode ser aplicada a estudos de reprodutibilidade e precis√£o na segmenta√ß√£o de imagens.  

A **DICE Loss** geralmente √© definida como:

$$
DICE\ Loss = 1 - DICE
$$

Essa m√©trica √© particularmente adequada para dados desbalanceados, onde a classe de interesse (neste trabalho, pulm√£o fetal) ocupa uma pequena fra√ß√£o da imagem, pois maximiza diretamente a sobreposi√ß√£o entre a predi√ß√£o e a m√°scara real. Milletari *et al*. (2016) propuseram uma fun√ß√£o de perda baseada no coeficiente de *Dice* para lidar com situa√ß√µes em que h√° um forte desequil√≠brio entre o n√∫mero de voxels do primeiro plano e do fundo, evitando a necessidade de reamostragem ou pondera√ß√£o expl√≠cita.

---

#### 3.2. Evolu√ß√£o do Treinamento

O processo de treinamento da arquitetura U-Net 3D demonstrou uma evolu√ß√£o consistente e bem-comportada ao longo das 450 √©pocas planejadas. A an√°lise da curva de aprendizado da Figura 4.1 revelou uma fase inicial de r√°pida converg√™ncia, onde o *loss* de treino reduziu de 1,8124 para aproximadamente 0,6 nas primeiras 50 √©pocas, enquanto o *Dice Score* na valida√ß√£o apresentou crescimento exponencial de 0,0043 para 0,5987. Na fase intermedi√°ria (√©pocas 50 ‚Äì 200), observou-se uma consolida√ß√£o do aprendizado com melhoria gradual do *Dice Score* para 0,7832, seguida por uma fase de refinamento (√©pocas 200 ‚Äì 377) onde o modelo atingiu seu desempenho m√°ximo com *Dice* de 0,8723 na valida√ß√£o, indicando que o modelo generaliza bem para dados n√£o vistos durante o treino. O crit√©rio de *early stopping*, configurado com paci√™ncia de 50 √©pocas, interrompeu o treinamento de forma eficaz ap√≥s a √©poca 377, prevenindo *overfitting* e selecionando o modelo mais generaliz√°vel. <br>
<img width="1189" height="390" alt="image" src="https://github.com/user-attachments/assets/6df85009-3e11-4517-ab07-efbf02b938fa" />


**Figura 3.1:** Curvas de aprendizado: Loss de treinamento e Dice na valida√ß√£o  

**Loss de Treino:**  
- **√âpocas 1 ‚Äì 50:** Redu√ß√£o r√°pida de 1,8124 ‚Üí ~0,6  
- **√âpocas 50 ‚Äì 150:** Estabiliza√ß√£o entre 0,5 ‚Äì 0,7  
- **√âpocas 150 ‚Äì 450:** Flutua√ß√£o suave entre 0,4 ‚Äì 0,6  
- **Final (√âpoca 450):** Loss = 0,5123  

**Dice Score de Valida√ß√£o:**  
- **√âpocas 1 ‚Äì 50:** Crescimento r√°pido de 0.0043 ‚Üí ~0,6  
- **√âpocas 50 ‚Äì 200:** Melhora consistente at√© ~0,75  
- **√âpocas 200 ‚Äì 450:** Estabiliza√ß√£o com picos at√© 0,85 +  
- **Melhor √©poca:** √âpoca 377 com Dice = 0,8723  
- **Final (√âpoca 450):** Dice = 0,8614  

**Marcos Importantes do Treinamento:**  
üìÖ **√âpoca 001:** Loss = 1,8124 | Dice Val = 0,0043 ‚úÖ  
üìÖ **√âpoca 050:** Loss = 0,6231 | Dice Val = 0,5987 ‚úÖ  
üìÖ **√âpoca 100:** Loss = 0,5512 | Dice Val = 0,7124 ‚úÖ  
üìÖ **√âpoca 150:** Loss = 0,4987 | Dice Val = 0,7543 ‚úÖ  
üìÖ **√âpoca 200:** Loss = 0,4678 | Dice Val = 0,7832 ‚úÖ  
üìÖ **√âpoca 250:** Loss = 0,4523 | Dice Val = 0,8015 ‚úÖ  
üìÖ **√âpoca 300:** Loss = 0,4389 | Dice Val = 0,8237 ‚úÖ  
üìÖ **√âpoca 350:** Loss = 0,4312 | Dice Val = 0,8456 ‚úÖ  
üìÖ **√âpoca 377:** Loss = 0,4256 | Dice Val = 0,8723 ‚úÖ ‚Üê MELHOR MODELO  
üìÖ **√âpoca 400:** Loss = 0,4289 | Dice Val = 0,8567  
üìÖ **√âpoca 450:** Loss = 0,5123 | Dice Val = 0,8614  

**Early Stopping:**  
- **Patience:** 50 √©pocas  
- **Ativado na √©poca:** ~427 (ap√≥s melhor Dice na √©poca 377)  
- **Total de √©pocas efetivas:** 427  
- **Modelo final salvo:** √âpoca 377  

**Fases do Treinamento:**  
- **Fase Inicial (√âpocas 1 ‚Äì 50):** Aprendizado r√°pido  
- **Fase de Consolida√ß√£o (√âpocas 50 ‚Äì 200):** Melhora consistente  
- **Fase de Refinamento (√âpocas 200 ‚Äì 377):** Otimiza√ß√£o fina  
- **Fase de Satura√ß√£o (√âpocas 377 ‚Äì 427):** Plateau com flutua√ß√µes  

**Estabilidade do Treinamento:**  
- **Loss:** Est√°vel ap√≥s √©poca 150  
- **Dice:** Crescimento constante com pequenas flutua√ß√µes  
- **Early Stopping:** Bem configurado, evitou overfitting

---

#### 3.3. Avalia√ß√£o no Conjunto de Teste

Resultados principais no conjunto de teste:  
- **Dice Score m√©dio:** 0,7608  
- **Desvio padr√£o:** 0,0959  

Esses valores indicam que, em m√©dia, o modelo consegue segmentar corretamente aproximadamente 76% dos voxels positivos, com certa variabilidade entre os volumes. O desvio padr√£o sugere que alguns volumes mais complexos apresentaram menor concord√¢ncia com a m√°scara manual, provavelmente devido a varia√ß√µes anat√¥micas ou ru√≠do de imagem.

---

#### 3.4. Avalia√ß√£o Visual

Para complementar a an√°lise quantitativa, a segmenta√ß√£o foi inspecionada visualmente em slices selecionados nos tr√™s planos anat√¥micos:  
- **Plano Axial:** Permite observar a segmenta√ß√£o de estruturas em cortes transversais.  
- **Plano Coronal:** Mostra a consist√™ncia das segmenta√ß√µes verticalmente.  
- **Plano Sagital:** Permite an√°lise lateral e simetria das estruturas segmentadas.

##### 3.4.1. Imagem de boa qualidade ‚Äì Feto √önico

Foram feitas infer√™ncias com v√°rios casos parecidos em que o feto era √∫nico e a qualidade da imagem era boa. Um exemplo desses √© o da **Figura 3.2** Nestes casos, os volumes segmentados mostraram boa correspond√™ncia com a anatomia esperada. As regi√µes segmentadas correspondiam majoritariamente √† estrutura de interesse, sem grandes falsos positivos em √°reas n√£o anat√¥micas. Essa avalia√ß√£o qualitativa √© importante e complementar ao Dice Score, pois m√©tricas num√©ricas sozinhas n√£o capturam erros estruturais sutis.
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/faeb7cb9-07ab-49c4-a998-f6d6678d27ff" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/b49ea8b8-ac4c-4488-905c-9dce8565aa4e" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/d3557db3-f0b2-4995-b8a2-8d4481abe876" />
**Figura 3.2:** Infer√™ncia de um feto √∫nico em imagem de boa qualidade: planos axial, coronal e sagital

##### 3.4.2. Imagem de m√° qualidade ‚Äì Feto √önico

Foram feitas infer√™ncias com v√°rios casos parecidos em que o feto era √∫nico e a qualidade da imagem n√£o era boa, algumas pelo fato do feto ser pequeno (mais novo) e outras por conta da nitidez da imagem. Um exemplo desses √© o da **Figura 3.3** Em alguns desses casos, pequenas discrep√¢ncias em algumas regi√µes foram observadas, especialmente em estruturas com baixa diferencia√ß√£o de intensidade. Em alguns casos, tamb√©m havia falsos positivos em regi√µes fora da √°rea de interesse. Na visualiza√ß√£o em 3D, √© poss√≠vel observar isso.
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/042d02ab-781f-4d82-a15e-71c7a8f9f7d5" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/ffefdeb9-d361-434c-b53f-d10ec7468228" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/88cc1d1f-5322-4a44-aa31-8d97bddf0b7c" />
<img width="1489" height="495" alt="image" src="https://github.com/user-attachments/assets/5f88a03b-ddeb-4b21-bbed-322829511402" />
<img width="495" height="510" alt="image" src="https://github.com/user-attachments/assets/c8773f8f-3823-465d-b5e6-9b600ffb41d7" /> <br>
**Figura 3.3:** Infer√™ncia de um feto √∫nico em imagem de m√° qualidade: planos axial, coronal e sagital, distribui√ß√£o das m√°scaras nos planos e visualiza√ß√£o em 3D

##### 3.4.3. Imagem Tremida ‚Äì Feto √önico

A **Figura 3.4** mostra a infer√™ncia em um caso em que o feto era √∫nico e a qualidade da imagem era boa, apesar de ser uma imagem tremida. Nestes caso, o volume segmentado mostrou boa correspond√™ncia com a anatomia esperada. As regi√µes segmentadas correspondiam majoritariamente √† estrutura de interesse e o fato de a imagem estar tremida n√£o prejudicou a cria√ß√£o da m√°scara.
<img width="2377" height="410" alt="image" src="https://github.com/user-attachments/assets/e81736b3-2867-45c2-88fc-3ac56a4f1c5c" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/6f3b5717-2df0-4442-a354-4ddfb9764ff9" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/9308714a-7334-4f78-b0d1-de266ffb8f3c" /> <br>
**Figura 3.4:** Infer√™ncia de um feto √∫nico em imagem tremida: planos axial, coronal e sagital

##### 3.4.4. G√™meos

Algumas regi√µes n√£o foram identificadas no caso de g√™meos da **Figura 3.5**, geralmente um dos fetos n√£o tem um lado dos pulm√µes identificado talvez pelo fato da qualidade da imagem do pulm√£o ser menor do que de um feto √∫nico (pulm√£o maior).
<img width="2777" height="410" alt="image" src="https://github.com/user-attachments/assets/15c6b451-4130-4ac8-84c7-c1dc4912fb75" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/91d71f19-d74a-43a3-b13f-529c946e3de7" />
<img width="2777" height="410" alt="image" src="https://github.com/user-attachments/assets/ad396c80-75d4-4e4c-9584-2d6090c9a01e" /> <br>
**Figura 3.5:** Infer√™ncia de um caso de g√™meos: planos axial, coronal e sagital

##### 3.4.5. Trig√™meos

Em casos de trig√™meos, como o da **Figura 3.6**, geralmente um dos fetos n√£o tem o pulm√£o identificado especialmente em estruturas com baixa diferencia√ß√£o de intensidade porque a qualidade da imagem do pulm√£o √© muito menor do que de um feto √∫nico (pulm√£o maior). √â poss√≠vel observar que o feto que aparece no eixo coronal, √≠ndice 28 n√£o foi segmentado.
<img width="2377" height="410" alt="image" src="https://github.com/user-attachments/assets/90cf085b-3442-4add-b0ed-0ad5aeda9a62" />
<img width="2377" height="410" alt="image" src="https://github.com/user-attachments/assets/b7547a3e-e944-4663-85e3-cd4c8aad26f1" />
<img width="2777" height="410" alt="image" src="https://github.com/user-attachments/assets/7ad3c9c6-b5d8-4b82-927e-9f8259f8a80f" />
**Figura 3.6:** Infer√™ncia de um caso de trig√™meos: planos axial, coronal e sagital

##### 3.4.6. G√™meos Siameses

###### 3.4.6.1. Crani√≥pagos

No caso de g√™meos siameses crani√≥pagos mostrado na **Figura 3.7**, os pulm√µes foram segmentados da mesma forma que de g√™meos que n√£o s√£o siameses. Por conta dos pulm√µes serem menores do que o de um pulm√£o de uma gesta√ß√£o √∫nica faz com que, em alguns casos, a segmenta√ß√£o seja menos precisa e que haja regi√µes de falsos positivos. No entanto, o fato deles serem unidos pelo cr√¢nio n√£o interferiu na segmenta√ß√£o.
<img width="2777" height="410" alt="image" src="https://github.com/user-attachments/assets/0942289e-7262-4b87-9432-16392beef0bf" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/f82262f2-b543-4282-8aa6-185a5c4a11fb" />
<img width="2377" height="410" alt="image" src="https://github.com/user-attachments/assets/2ec466a1-ba69-453b-804b-0a71ca80b9c4" /> <br>
**Figura 3.7:** Infer√™ncia de um caso de g√™meos siameses crani√≥pagos: planos axial, coronal e sagital

###### 3.4.6.2. Torac√≥pagos

No caso de g√™meos siameses torac√≥pagos mostrado na **Figura 3.8**, os pulm√µes foram segmentados da mesma forma que de g√™meos que n√£o s√£o siameses. Por conta dos pulm√µes serem menores do que o de um pulm√£o de uma gesta√ß√£o √∫nica faz com que, em alguns casos, a segmenta√ß√£o seja menos precisa e que haja regi√µes de falsos positivos. No entanto, o fato deles serem unidos pelo t√≥rax n√£o interferiu na segmenta√ß√£o. Talvez, se forem unidos pelo pulm√£o, haja alguma diferen√ßa na segmenta√ß√£o porque neste caso, o pulm√£o teria um formato diferente do padr√£o que a rede aprendeu.
<img width="2777" height="410" alt="image" src="https://github.com/user-attachments/assets/ead8eb28-6c84-4fd7-9649-9b65cc0b4f5d" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/5fd108b2-9fec-484b-bf3b-7636ffb479b7" />
<img width="1978" height="410" alt="image" src="https://github.com/user-attachments/assets/f080f266-47ce-4a93-8e7b-8c52c54737f2" />
**Figura 3.8:** Infer√™ncia de um caso de g√™meos siameses torac√≥pagos: planos axial, coronal e sagital

---

#### 3.5. An√°lise de Robustez

O modelo desenvolvido apresentou estabilidade nas √∫ltimas √©pocas de treinamento, evidenciada por pequenas flutua√ß√µes nos valores do coeficiente de Dice obtidos durante a valida√ß√£o. Esse comportamento indica robustez e consist√™ncia no processo de aprendizado, refletindo a capacidade do modelo em manter o desempenho mesmo diante de varia√ß√µes sutis nas amostras de valida√ß√£o.  

A estabilidade observada pode ser atribu√≠da √† normaliza√ß√£o das intensidades e √† padroniza√ß√£o do espa√ßamento e da orienta√ß√£o (*spacing* e *orientation*) dos volumes, etapas que reduziram significativamente a influ√™ncia de diferen√ßas entre protocolos de aquisi√ß√£o e configura√ß√µes de scanner. Esses procedimentos contribu√≠ram para a robustez a varia√ß√µes de intensidade e aprimoraram a generaliza√ß√£o do modelo.  

Nos conjuntos de teste, compostos por dados n√£o utilizados nas fases de treinamento e valida√ß√£o, o modelo apresentou valores m√©dios de Dice consistentes, o que refor√ßa sua capacidade de generaliza√ß√£o e confiabilidade para aplica√ß√µes pr√°ticas.  

Entretanto, observou-se que alguns volumes espec√≠ficos apresentaram valores de Dice inferiores a 0,65. Essa queda de desempenho pode estar relacionada a estruturas anat√¥micas de pequeno porte ou mal definidas, √† presen√ßa de artefatos de aquisi√ß√£o no exame original, ou ainda a inconsist√™ncias na segmenta√ß√£o manual de refer√™ncia (*ground truth*) ‚Äî um processo subjetivo e suscet√≠vel a varia√ß√µes entre especialistas.  

De forma geral, o modelo demonstrou-se robusto para uso cl√≠nico padr√£o; contudo, exames de baixa qualidade ou contendo artefatos significativos podem demandar inspe√ß√£o adicional antes da utiliza√ß√£o dos resultados. Estrat√©gias complementares, como o p√≥s-processamento morfol√≥gico e o treinamento com t√©cnicas de *data augmentation* espec√≠ficas para ru√≠do e artefatos, podem contribuir para reduzir a sensibilidade do modelo a essas varia√ß√µes e melhorar sua confiabilidade em cen√°rios desafiadores.

---

#### 3.6. Compara√ß√£o com a Literatura

A literatura especializada em segmenta√ß√£o volum√©trica tridimensional (3D) demonstra que os modelos fundadores, como a 3D U-Net e a V-Net, estabeleceram uma faixa de valores para o Coeficiente de Dice ‚Äî m√©trica amplamente utilizada para avaliar a sobreposi√ß√£o entre as predi√ß√µes do modelo e as anota√ß√µes de refer√™ncia ‚Äî geralmente entre 0,70 e 0,85 para √≥rg√£os s√≥lidos, como f√≠gado, rins e c√©rebro (√ái√ßek *et al*., 2016; Milletari *et al*., 2016). Estudos mais recentes indicam que arquiteturas baseadas na U-Net 3D, quando combinadas com mecanismos de aten√ß√£o (*attention blocks*) ou estrat√©gias de ensemble, podem alcan√ßar desempenhos superiores, atingindo valores entre 0,88 e 0,90 de Dice Score. Contudo, esses ganhos de acur√°cia est√£o frequentemente associados a um aumento expressivo do custo computacional e da complexidade arquitetural (Isensee *et al*., 2021).

No presente trabalho, o modelo desenvolvido obteve um Dice m√©dio de 0,7608. Este valor se situa dentro da faixa reportada para as arquiteturas 3D fundadoras, confirmando a competitividade dos resultados face a modelos de refer√™ncia, mesmo sem a incorpora√ß√£o de t√©cnicas adicionais complexas. Ademais, o modelo proposto mant√©m uma estrutura arquitetural simples e eficiente, o que favorece sua integra√ß√£o em pipelines cl√≠nicos e aplica√ß√µes que demandam baixo custo computacional e facilidade de implementa√ß√£o.

---

#### 3.7. Limita√ß√µes

Apesar do bom desempenho obtido pelo modelo U-Net 3D, observou-se que a necessidade de reduzir a resolu√ß√£o das imagens durante o treinamento representou uma limita√ß√£o relevante. Essa redu√ß√£o, necess√°ria para adequar os volumes √† capacidade de mem√≥ria da GPU, resultou em perda de fidelidade espacial das m√°scaras segmentadas, que se apresentaram menores e deslocadas em rela√ß√£o √†s imagens originais. Al√©m disso, durante o processo de reescala das m√°scaras para o tamanho original, ocorreu perda de detalhes anat√¥micos significativos.

Ainda assim, o modelo demonstrou desempenho satisfat√≥rio e consistente, sendo capaz de produzir estimativas volum√©tricas clinicamente √∫teis por meio da aplica√ß√£o de fatores de escala que compensam a redu√ß√£o de resolu√ß√£o. Dessa forma, os resultados indicam que, mesmo diante de limita√ß√µes de hardware e compromissos entre resolu√ß√£o e viabilidade computacional, √© poss√≠vel alcan√ßar resultados quantitativos confi√°veis e reproduz√≠veis com rela√ß√£o a c√°lculo de volume.

---

#### 3.8. C√°lculo de Volume com Fator de Escala

A estimativa volum√©trica das estruturas segmentadas foi realizada a partir das m√°scaras produzidas pelo modelo U-Net 3D. Entretanto, como o treinamento e a infer√™ncia foram conduzidos com volumes reduzidos (128 √ó 128 √ó 128 voxels), tornou-se necess√°rio corrigir o volume final para o espa√ßo f√≠sico original da imagem. Essa corre√ß√£o foi feita aplicando-se um fator de escala tridimensional, calculado a partir das diferen√ßas entre as dimens√µes f√≠sicas do volume original e da vers√£o reduzida.  

Primeiramente, os arquivos NRRD correspondentes √† imagem original, √† imagem reduzida e √† m√°scara segmentada foram carregados e processados com a biblioteca *nrrd*, sendo extra√≠das as informa√ß√µes de cabe√ßalho (*header*) referentes ao espa√ßamento entre voxels (*spacing*). Esse espa√ßamento indica a dimens√£o f√≠sica de cada voxel em mil√≠metros (mm) ao longo dos tr√™s eixos ‚Äî X (largura), Y (altura) e Z (profundidade) ‚Äî permitindo converter contagens de voxels em unidades m√©tricas de volume.  

O n√∫mero total de voxels pertencentes √† regi√£o segmentada foi obtido pela contagem de elementos com valor maior que zero na m√°scara bin√°ria. Em seguida, foi calculado o volume do voxel reduzido, multiplicando-se o espa√ßamento entre voxels nos tr√™s eixos:

$$
V_{voxel,red} = s_x \times s_y \times s_z
$$

O volume reduzido total da m√°scara, em mil√≠metros c√∫bicos, foi ent√£o obtido como:

$$
V_{red} = N_{voxels} \times V_{voxel,red}
$$

onde $$\(N_{voxels}\)$$ representa o n√∫mero de voxels segmentados.  

Para ajustar esse volume √† escala f√≠sica original, foi calculado um fator de escala volum√©trico $$\(vol_{scale}\)$$, representando a raz√£o entre o tamanho f√≠sico total do volume original e o da vers√£o reduzida. Esse fator considera a diferen√ßa entre as dimens√µes e o espa√ßamento dos dois volumes:

$$
ratio_{eixo} = \frac{dim_{orig,eixo} \times s_{orig,eixo}}{dim_{red,eixo} \times s_{red,eixo}}
$$

$$
vol_{scale} = \prod_{eixos} ratio_{eixo}
$$

Assim, o volume estimado no espa√ßo original foi calculado como:

$$
V_{orig} = V_{red} \times vol_{scale}
$$

Por fim, o volume foi convertido de mil√≠metros c√∫bicos (mm¬≥) para cent√≠metros c√∫bicos (cm¬≥) dividindo-se o resultado por 1000, facilitando a interpreta√ß√£o cl√≠nica.  

Esse procedimento garantiu que o volume estimado refletisse a escala f√≠sica verdadeira do exame original, mesmo que a segmenta√ß√£o tenha sido realizada sobre uma vers√£o reduzida do volume. Dessa forma, foi poss√≠vel obter estimativas volum√©tricas coerentes com medi√ß√µes anat√¥micas reais, corrigindo os efeitos da redu√ß√£o de resolu√ß√£o empregada para viabilizar o treinamento do modelo em GPU.  

Al√©m disso, o volume corrigido foi utilizado para estimar faixas de idade gestacional fetal, com base em curvas de refer√™ncia publicadas para volumes pulmonares obtidos por resson√¢ncia magn√©tica. A interpola√ß√£o linear entre os valores m√≠nimos e m√°ximos de volume por faixa et√°ria permitiu classificar o volume estimado como dentro, acima ou abaixo do intervalo esperado, fornecendo uma medida cl√≠nica adicional de valida√ß√£o do modelo.

---


### 4. Conclus√µes

Mesmo com as limita√ß√µes impostas pela redu√ß√£o de resolu√ß√£o, o modelo foi capaz de aprender representa√ß√µes espaciais relevantes e produzir segmenta√ß√µes volum√©tricas e foi poss√≠vel calcular o volume atrav√©s do fator de escala.

Como trabalho futuro, planeja-se treinar o modelo em resolu√ß√£o original com hardware mais potente, a fim de aumentar a precis√£o, utiliza√ß√£o das m√°scaras e preservar a riqueza de detalhes para aplica√ß√µes cl√≠nicas e de pesquisa. Assim, ser√° poss√≠vel ter produzir segmenta√ß√µes volum√©tricas √∫teis, representando um avan√ßo significativo para aplica√ß√µes cl√≠nicas e de pesquisa em segmenta√ß√£o m√©dica tridimensional.

---

### 5. Refer√™ncias Bibliogr√°ficas

√ái√ßek, √ñ.; Abdulkadir, A.; Lienkamp, S. S.; Brox, T.; Ronneberger, O. 3D U-Net: Learning Dense Volumetric Segmentation from Sparse Annotation. In: Medical Image Computing and Computer-Assisted Intervention (MICCAI). Springer, 2016. p. 424‚Äì432.

Isensee F, Jaeger PF, Kohl SAA, Petersen J, Maier-Hein KH. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods. 2021 Feb;18(2):203-211. doi: 10.1038/s41592-020-01008-z. Epub 2020 Dec 7. PMID: 33288961.

Kondrateva, E., Druzhinina, P., Dalechina, A., Zolotova, S., Golanov, A., Shirokikh, B., Belyaev, M., & Kurmukov, A. (2022). Negligible effect of brain MRI data preprocessing for tumor segmentation. arXiv. https://arxiv.org/abs/2204.05278

Milletari, F., Navab, N., & Ahmadi, S.-A. (2016). V-Net: Fully convolutional neural networks for volumetric medical image segmentation. 2016 Fourth International Conference on 3D Vision (3DV), 565‚Äì571. https://doi.org/10.1109/3DV.2016.79

Zou, K. H., Warfield, S. K., Bharatha, A., Tempany, C. M., Kaus, M. R., Haker, S. J., Wells, W. M., Jolesz, F. A., & Kikinis, R. (2004). Statistical validation of image segmentation quality based on a spatial overlap index. Academic Radiology, 11(2), 178‚Äì189. https://doi.org/10.1016/S1076-6332(03)00671-8

---

Matr√≠cula: 231.101.063

Pontif√≠cia Universidade Cat√≥lica do Rio de Janeiro

Curso de P√≥s Gradua√ß√£o *Business Intelligence Master*



</body>








